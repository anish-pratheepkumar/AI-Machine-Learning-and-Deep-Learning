# Deep Learning - Course by Prof MÃ¼ller
The exercises are based on the Deep Learning framework - Pytorch. Starting with introduction to python the exercises moves to implementing forward and backward propagation from scratch and in the end training MNIST and CIFAR10 data sets using CNN.

### Exercise_1_[Intro_Python&NumPy](https://github.com/anish-pratheepkumar/GitDeepLearningStudy/tree/master/DeepLearningCourse-Uni/Exercise_1_Intro_Python%26NumPy)

This exercise focuses on familiarising the Python programming language and also understanding NumPy library.

### Exercise_2_[WineRegression](https://github.com/anish-pratheepkumar/GitDeepLearningStudy/tree/master/DeepLearningCourse-Uni/Exercise_2_WineRegression)

A regression problem is solved using wine data(3674 training data) in a .csv file, having 10 features of the wine and corresponding quality. Here a regression model is developed which can predict the quality of the wine when given the 10 input features.

### Exercise_3_[forward from scratch](https://github.com/anish-pratheepkumar/GitDeepLearningStudy/tree/master/DeepLearningCourse-Uni/Exercise_3_forward%20from%20scratch)

Here the forward propagation concept in Neural Network is implemented from scratch using Python.

### Exercise_4_[GradientDescent(Autograd)](https://github.com/anish-pratheepkumar/GitDeepLearningStudy/tree/master/DeepLearningCourse-Uni/Exercise_4_GradientDescent(Autograd))

Next the back propagation is implemented from scratch using Python. This is similar to Autograd in Pytorch.

### Exercise_5_[SGDfromScratch](https://github.com/anish-pratheepkumar/GitDeepLearningStudy/tree/master/DeepLearningCourse-Uni/Exercise_5_SGDfromScratch)

The Stochastic Gradient Descent(SGD) optimisation is implemented from scratch. Also an introduction to PYtorch is given in this Exercise.

### Exercise_6_[Initialisation&BatchNormalisation](https://github.com/anish-pratheepkumar/GitDeepLearningStudy/tree/master/DeepLearningCourse-Uni/Exercise_6_Initialisation%26BatchNormalisation)

First section gives information regarding the parameter initialisation of a Network. Followed by implementation of Batch Normalisation.

### Exercise_7_[Dropout](https://github.com/anish-pratheepkumar/GitDeepLearningStudy/tree/master/DeepLearningCourse-Uni/Exercise_7_Dropout)

The Dropout regularisation is implemented using Python.

### Exercise_8.0_[Convolutions](https://github.com/anish-pratheepkumar/GitDeepLearningStudy/tree/master/DeepLearningCourse-Uni/Exercise_8.0_Convolutions)

The Convolutional Neural Network implemnetation using Python and also a look at the edge detector and Gaussian Filter.

### Exercise_8.1_[MNIST&CIFAR10](https://github.com/anish-pratheepkumar/GitDeepLearningStudy/tree/master/DeepLearningCourse-Uni/Exercise_8.1_MNIST%26CIFAR10)

The MNIST and CIFAR10 datasets are trained using Pytorch framework and the results are presented.

### Exercise_8.2_[VGG_FCN2CNN](https://github.com/anish-pratheepkumar/GitDeepLearningStudy/tree/master/DeepLearningCourse-Uni/Exercise_8.2_VGG_FCN2CNN)

The concept of replacing the Fully Conected Layers (at the end of a CNN) to  Convlutional layers is implemented here. 





