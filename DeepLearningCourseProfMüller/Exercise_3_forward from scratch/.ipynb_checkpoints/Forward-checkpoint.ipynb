{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Important for reloading modified files\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "from modules import Network, LinearLayer, Sigmoid, ReLU, MSE, CrossEntropyLoss\n",
    "\n",
    "import codecs, json "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you will implement function evaluations for some of the most fundamental building blocks of modern arificial neural networks. The idea is that you create a network consisting of several layers each of which implements the `forward` function inherited from the base class `Module`. A skeleton for your implementations is provided by `modules.py`. Work through this notebook to validate your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in the lecture, the notion of a *layer* is not well-defined and we may even regard a whole network as a layer predicting a desired output from input data. Therefore we want our class ```Network``` to be a subclass of ```Module```. Of course, we need a mechanism for implementing this high-level view of mapping input to output. This is achieved by sequential execution of certain network layers, which we want to store in our ```Network``` class. Follow the comments below and complete the code. You can test your implementation at the end of this notebook after finishing the other tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, layers are the building blocks of neural networks. In this section, you will familiarize yourself with some of the most common types of layers in artificial neural networks.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear layers are a simple and yet extremely powerful tool for the design of deep neural networks. This is partly due to the highly parallelized and thus efficient computation of linear transformations on modern GPUs. Implement the class `LinearLayer` and test you implementation by running the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.ones((3, 4))\n",
    "b = np.linspace(1, 3, 3)\n",
    "\n",
    "ll = LinearLayer(W, b)\n",
    "x = np.ones(4)\n",
    "\n",
    "assert np.abs(np.max(ll.forward(x) - [5, 6, 7])) < 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sigmoid activation has been of great importance in the field of machine learning ever since. Biologically motivated at first, it carries out the mapping $x \\mapsto {e^x} / (1 + e^x)$. Complete the forward pass of the class `Sigmoid`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.abs(np.max(Sigmoid().forward([0, -1, 10]) - [0.5, 0.2689414, 1.0])) < 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaining tremendous success in the last couple of years, **Re**ctified __L__inear **U**nits are now the most common activations in use. Their power comes from the simplicity of their forward pass, which discards all negative values, i.e. $x \\mapsto \\max(x, 0)$, where the maximum is to be understood element-wise. \n",
    "\n",
    "Test your implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.abs(np.max(ReLU().forward([-3.14, 0, 1, 10]) - [0., 0., 1., 10])) < 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task of a network is predicting a desired output from input data. The quality of the prediction is assessed by *loss functions* comparing the predicted output with the target or groundtruth. In our implementation we can model loss as subclass of Module. It therefore also features a forward function. However, we now require it to take an argument for the output of the network **and** an argument for the target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already discussed the MSE loss in the context of linear regression. Implement the `forward` function calculating the mean squared difference of prediction and target.\n",
    "\n",
    "Test your implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.abs(np.max(MSE().forward(np.array([0., 1., 2., 1.5]), np.array([0., 1., 1., -1.])) - 7.25/4)) < 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another important task in the regime of machine learning is *classification*. Given some input $x$ we want to predict a discrete class label $l\\in\\{1, \\ldots, L\\}$. In order to train neural networks, we need a differentiable forward pass, which renders a discrete prediction function impossible. Therefore, we want to predict a vector in $\\mathbb{R}^L$ representing our believe for each label. We can actually transform this vector into a valid probability distribution using the softmax function (https://en.wikipedia.org/wiki/Softmax_function)\n",
    "$$\n",
    "\\sigma \\, \\colon \\, \\mathbb{R}^L \\to \\left\\{ \\sigma \\in \\mathbb{R}^L \\, \\middle| \\, \\sigma_i > 0, \\sum_{i=1}^L \\sigma_i = 1 \\right\\}, \\, \\sigma_j ( z ) = \\frac{e^{z_j}}{\\sum_{i=1}^L e^{z_i}} \\text{ for $j \\in \\left\\{ 1, \\ldots, L \\right\\}$}.\n",
    "$$\n",
    "\n",
    "This in turn allows us to define a proper loss function: we simply take the negative log of the predicted probability of the target label $l$, i.e. $\\ell (x, l) = -\\log (\\sigma_l (x))$. Implement the cross entropy loss, where $x$ is the prediction of our network and $l$ is given by the target label.\n",
    "\n",
    "Test your implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "assert np.abs(np.max(CrossEntropyLoss().forward(np.array([-3.14, 0, 1, 10]), 0) - 13.1401)) < 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final test of the network implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = np.ones((3, 4))\n",
    "b1 = np.linspace(1, 3, 3)\n",
    "ll1 = LinearLayer(W1, b1)\n",
    "\n",
    "W2 = np.ones((1, 3))\n",
    "b2 = np.ones((1))\n",
    "ll2 = LinearLayer(W2, b2)\n",
    "\n",
    "relu = ReLU()\n",
    "\n",
    "net = Network([ll1, relu])\n",
    "\n",
    "net.add_layer(ll2)\n",
    "\n",
    "assert np.abs(np.max(net.forward(np.array([-3.14, 0, 1, 10]).T) - 30.58)) < 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Task: XOR*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In 1969, Minsky and Papert pointed out that first approaches to artificial neural networks in the 60's weren't able to resemble the XOR function: given two booleans $x_1, x_2 \\in \\{0, 1\\}$, the XOR gate outputs $1$ if exactly one of the two booleans is $1$ and $0$ otherwise. Using the tools from above, can you design a handcrafted network $\\mathcal{N} \\colon \\mathbb{R}^2 \\to \\mathbb{R}$ overcoming this problem? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (2,2) and (1,) not aligned: 2 (dim 1) != 1 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-e0f4ed0c8e8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLinearLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"not quite...\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"not quite...\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"not quite...\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Jupyter Notebook/DL Müller/Ex3/Exercise_3/modules.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# single layer and pass the output as input to the next layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Jupyter Notebook/DL Müller/Ex3/Exercise_3/modules.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m# compute the linear transformation x -> Wx + b\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (2,2) and (1,) not aligned: 2 (dim 1) != 1 (dim 0)"
     ]
    }
   ],
   "source": [
    "nn = Network()\n",
    "\n",
    "###########################\n",
    "\n",
    "# Define W1, b1, W2, b2\n",
    "# in such a way that nn\n",
    "# resemles XOR\n",
    "\n",
    "W1 = np.matrix([[1., -1.], [-1., 1.]])\n",
    "b1 = np.zeros((2))\n",
    "W2 = np.ones((2))\n",
    "b2 = np.zeros((1))\n",
    "\n",
    "###########################\n",
    "\n",
    "nn.add_layer(LinearLayer(W1, b1))\n",
    "nn.add_layer(ReLU())\n",
    "nn.add_layer(LinearLayer(W2, b2))\n",
    "\n",
    "assert nn.forward([0, 0]) == 0, \"not quite...\"\n",
    "assert nn.forward([1, 0]) == 1, \"not quite...\"\n",
    "assert nn.forward([0, 1]) == 1, \"not quite...\"\n",
    "assert nn.forward([1, 1]) == 0, \"not quite...\"\n",
    "\n",
    "# storing your parameters to \"val.json\"\n",
    "val = [W1.tolist(), b1.tolist(), W2.tolist(), b2.tolist()]\n",
    "json.dump(val, codecs.open(\"val.json\", 'w', encoding='utf-8'), separators=(',', ':'), sort_keys=True, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
