{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture you have already discussed the gradient backpropagation algorithm for calculating derivatives in neural networks. In this exercise you will implement this algorithm in a similar fashion as it is done in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import nn\n",
    "import toolbox as tb\n",
    "from toolbox import Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate the general idea, run the following lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Tensor(1., requires_grad=True)\n",
    "b = Tensor(2., requires_grad=True)\n",
    "\n",
    "c = a + b\n",
    "d = c + a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "2.0\n"
     ]
    }
   ],
   "source": [
    "print(d.grad)\n",
    "print(c.grad)\n",
    "print(b.grad)\n",
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to understand what's going on here. After we have run ``d.backward()`` the variables ``a``, ``b`` and ``c`` involved in the computation of ``d`` now feature an attribute ``grad``. This attribute represents the gradient of each variable w.r.t. computing ``d``:\n",
    "$$\n",
    "\\frac{\\mathrm d \\mathop{}\\! d}{\\mathrm d \\mathop{}\\! c} = \\frac{\\mathrm d \\mathop{}\\! c}{\\mathrm d \\mathop{}\\! c} + \\frac{\\mathrm d \\mathop{}\\! a}{\\mathrm d \\mathop{}\\! c} = 1 + 0 = 1, \\\\\n",
    "\\frac{\\mathrm d \\mathop{}\\! d}{\\mathrm d \\mathop{}\\! b} = \\frac{\\mathrm d \\mathop{}\\! c}{\\mathrm d \\mathop{}\\! b} + \\frac{\\mathrm d \\mathop{}\\! a}{\\mathrm d \\mathop{}\\! b} = 1 + 0 = 1, \\\\\n",
    "\\frac{\\mathrm d \\mathop{}\\! d}{\\mathrm d \\mathop{}\\! a} = \\frac{\\mathrm d \\mathop{}\\! c}{\\mathrm d \\mathop{}\\! a} + \\frac{\\mathrm d \\mathop{}\\! a}{\\mathrm d \\mathop{}\\! a} = \\left(\\frac{\\mathrm d \\mathop{}\\! a}{\\mathrm d \\mathop{}\\! a} + \\frac{\\mathrm d \\mathop{}\\! b}{\\mathrm d \\mathop{}\\! a}\\right) + \\frac{\\mathrm d \\mathop{}\\! a}{\\mathrm d \\mathop{}\\! a} = \\left( 1 + 0 \\right) + 1 = 2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But how does ``a`` know about the computations going on after its creation? The answer is that every operation we apply to tensors creates a new object representing this operation with all its variables involved. The output tensor of these operations saves a reference to the the operation which created it in the attribute ``grad_fn``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<toolbox.Add object at 0x7f26e55a9250>\n",
      "<toolbox.Add object at 0x7f26e55a99d0>\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "c = a + b\n",
    "d = c + a\n",
    "\n",
    "print(d.grad_fn)\n",
    "print(c.grad_fn)\n",
    "print(b.grad_fn)\n",
    "print(a.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the output above you can see that ``c`` and ``d`` resulted from addition operations. Variables ``a`` and ``b`` were created from scratch and therfore have the value ``None`` as ``grad_fn``. This mechnanism implicitly builds a computation graph with all the dependencies among the variables. The function call ``d.backward()`` is a shorhand for ``d.backward(1.)`` and starts the gradient backpropagation involving all variables involved in the creation of ``d``. The only exception are tensors for which the ``requires_grad`` evaluates to ``False``, which is also the default value, if not stated differently, for the cration of a tensor object.\n",
    "\n",
    "Let's try to call the ``backward`` function two times in a row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'grad_fn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-d6d913aeb46d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/Jupyter Notebook/DL MÃ¼ller/Ex4/Exercise_4/toolbox.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, dLdt)\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_forward\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_forward\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'grad_fn'"
     ]
    }
   ],
   "source": [
    "a = Tensor(1., requires_grad=True)\n",
    "b = Tensor(2., requires_grad=True)\n",
    "\n",
    "c = a + b\n",
    "d = c + a\n",
    "\n",
    "d.backward()\n",
    "d.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error is thrown, because the computation graph is deleted after the first gradient backpropagation as most of the times there is no use in keeping this graph alive and memory is a valuable good in the field of Deep Learning. Therefore a new computation graph is built for ever forward pass.\n",
    "\n",
    "Also observe the behaviour of the gradient calculation below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n",
      "2.0\n",
      "4.0\n"
     ]
    }
   ],
   "source": [
    "a = Tensor(1., requires_grad=True)\n",
    "b = Tensor(2., requires_grad=True)\n",
    "\n",
    "c = a + b\n",
    "d = c + a\n",
    "\n",
    "d.backward()\n",
    "\n",
    "c = a + b\n",
    "d = c + a\n",
    "\n",
    "d.backward()\n",
    "\n",
    "print(d.grad)\n",
    "print(c.grad)\n",
    "print(b.grad)\n",
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradients are accumulated over time and hence the values above are incorrect. Thus it will be important to reset the gradient values for parameters we want to optimize over to zero after each forward pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "2.0\n"
     ]
    }
   ],
   "source": [
    "a = Tensor(1., requires_grad=True)\n",
    "b = Tensor(2., requires_grad=True)\n",
    "\n",
    "c = a + b\n",
    "d = c + a\n",
    "\n",
    "d.backward()\n",
    "a.zero_grad()\n",
    "b.zero_grad()\n",
    "\n",
    "c = a + b\n",
    "d = c + a\n",
    "\n",
    "d.backward()\n",
    "\n",
    "print(d.grad)\n",
    "print(c.grad)\n",
    "print(b.grad)\n",
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at the file ``toolbox.py`` and make sure you understand the basic mechanism of how it implements the creation of computation graphs. Then follow the comments and implement the backward functions for each operation. Check you implementations by running the cells below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.0\n",
      "4.0\n",
      "2.0\n"
     ]
    }
   ],
   "source": [
    "a = Tensor(2., requires_grad=True)\n",
    "b = Tensor(3., requires_grad=True)\n",
    "\n",
    "c = a * b\n",
    "c.backward(2.)\n",
    "print(a.grad)\n",
    "print(b.grad)\n",
    "print(c.grad)\n",
    "assert np.abs(a.grad-6.)<1e-6 and np.abs(b.grad-4.)<1e-6, \"Multiplication doesn't work properly!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Tensor(np.array([1., 2.]), requires_grad=True)\n",
    "b = Tensor(np.array([3., 4.]), requires_grad=True)\n",
    "\n",
    "c = a / b\n",
    "c.backward(0.5)\n",
    "\n",
    "assert np.linalg.norm(a.grad-np.array([1./6, 1./8]))<1e-6 and \\\n",
    "np.linalg.norm(b.grad-np.array([-1./18, -1./16]))<1e-6, \\\n",
    "\"Division doesn't work properly!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = Tensor(np.array([[1., 2.], [3., 4.]]), requires_grad=True)\n",
    "\n",
    "b = A ** 4\n",
    "b.backward(np.array([[-1., 0], [1., 2.]]))\n",
    "\n",
    "assert np.linalg.norm(A.grad-np.array([[-4., 0.], [108., 512.]]))<1e-6, \"Exponentiation doesn't work properly!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -4.5  -9. ]\n",
      " [ -6.  -12. ]]\n",
      "[[ -7.5 -16.5]]\n",
      "[[-22.5]\n",
      " [-33. ]]\n",
      "[[-1.5]]\n"
     ]
    }
   ],
   "source": [
    "A = Tensor(np.array([[1., 2.], [3., 4.]]), requires_grad=True)\n",
    "b = Tensor(np.array([[1.], [2.]]), requires_grad=True)\n",
    "c = Tensor(np.array([[3., 4.]]), requires_grad=True)\n",
    "\n",
    "d = c @ A @ b\n",
    "d.backward(-1.5)\n",
    "print(A.grad)\n",
    "print(c.grad)\n",
    "print(b.grad)\n",
    "print(d.grad)\n",
    "assert np.linalg.norm(A.grad-np.array([[-4.5, -9.], [-6., -12.]]))<1e-6 and \\\n",
    "np.linalg.norm(b.grad-np.array([[-22.5], [-33.]]))<1e-6 and \\\n",
    "np.linalg.norm(c.grad-np.array([[-7.5, -16.5]]))<1e-6, \"Matrix multiplication doesn't work properly!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = Tensor(np.array([[-1., -2,], [0.5, 2.], [1., 5.]]), requires_grad=True)\n",
    "\n",
    "b = tb.relu(A)\n",
    "b.backward(np.array([[4., 3.], [2., 1.], [0., -1.]]))\n",
    "\n",
    "assert np.linalg.norm(A.grad-np.array([[0., 0.], [2., 1.], [0., -1.]]))<1e-6, \"ReLU doesn't work properly!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = Tensor(np.array([[-1.], [2.]]), requires_grad=True)\n",
    "\n",
    "b = tb.exp(A)\n",
    "b.backward(np.array([[-1.], [2.]]))\n",
    "\n",
    "assert np.linalg.norm(A.grad-np.exp(A.data)*np.array([[-1.], [2.]]))<1e-6, \\\n",
    "\"Exponential function doesn't work properly!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = Tensor(np.array([0.1, 1., 2., np.exp(100.)]), requires_grad=True)\n",
    "\n",
    "b = tb.log(A)\n",
    "b.backward(1.5)\n",
    "\n",
    "assert np.linalg.norm(A.grad-np.array([15., 1.5, 0.75, 1.5*np.exp(-100.)]))<1e-6, \"Logarithm doesn't work properly!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = Tensor(np.array([[1., 2.], [3., 4.]]), requires_grad=True)\n",
    "\n",
    "b = A.sum(dim=1)\n",
    "b = b.sum()\n",
    "b.backward(2.5)\n",
    "\n",
    "assert np.linalg.norm(A.grad-2.5*np.ones((2, 2)))<1e-6, \"Summation doesn't work properly!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = Tensor(np.array([[1., 2.], [3., 4.]]), requires_grad=True)\n",
    "\n",
    "b = A.mean()\n",
    "b.backward()\n",
    "\n",
    "assert np.linalg.norm(A.grad-0.25*np.ones((2, 2)))<1e-6, \"Mean doesn't work properly!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = Tensor(np.array([[1., 2.], [3., 4.]]), requires_grad=True)\n",
    "\n",
    "b = A[0, 1]\n",
    "b.backward(5.)\n",
    "\n",
    "assert np.linalg.norm(A.grad-5.*np.array([[0., 1.], [0., 0.]]))<1e-6, \"Indexing doesn't work properly!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the above test were successful, you should be able to backpropagate through a network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = nn.Network()\n",
    "network.add_layer(nn.LinearLayer(np.array([[1., 2.], [3., 4.]]), \\\n",
    "                                np.array([[1.], [2.]])))\n",
    "network.add_layer(nn.ReLU())\n",
    "network.add_layer(nn.LinearLayer(np.array([[1., 2.], [3., 4.], [5., 6.]]), \\\n",
    "                 np.array([[1.], [2.], [3.]])))\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "data = Tensor(np.array([[1.], [2.]]))\n",
    "target = 1\n",
    "\n",
    "l = loss.forward(network.forward(data), target)\n",
    "l.backward()\n",
    "\n",
    "assert np.linalg.norm(network.layers[0].W.grad-np.array([[2., 4.], [2., 4.]]))<1e-6 and \\\n",
    "np.linalg.norm(network.layers[0].b.grad-np.array([[2.], [2.]]))<1e-6 and \\\n",
    "np.linalg.norm(network.layers[2].W.grad-np.array([[ 8.00168889e-34,  1.73369926e-33], \\\n",
    "                                                  [-6.00000000e+00, -1.30000000e+01], \\\n",
    "                                                  [ 6.00000000e+00,  1.30000000e+01]]))<1e-6 and \\\n",
    "np.linalg.norm(network.layers[2].b.grad-np.array([[ 1.33361482e-34], \\\n",
    "                                                  [-1.00000000e+00], \\\n",
    "                                                  [ 1.00000000e+00]]))<1e-6, \"Something is wrong...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
