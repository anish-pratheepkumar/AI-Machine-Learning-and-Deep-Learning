{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last exercise we already discussed the conceptual mechanisms of gradient backpropagation and implemented some functionality in the ``toolbox`` module. This exercise introduces PyTorch, a Deep Learning framework providing powerful tools for simple and efficient implementations of neural networks.\n",
    "\n",
    "First, we need to import the corresponding Python module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic building block of PyTorch are Tensors, which behave very similar to the class ``toolbox.Tensor`` from last week. Tensors can be initialized by passing a NumPy array or by calling dedicated functions with syntax similar to NumPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor(np.array([[1., 2.], [3., 4.]]))\n",
    "b = torch.ones(2, 2)\n",
    "c = torch.empty(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]], dtype=torch.float64)\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([-2.2142e-01,  4.5594e-41,  1.5743e-37,  0.0000e+00,         nan])\n"
     ]
    }
   ],
   "source": [
    "print(a)\n",
    "print(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch features the same mechanism for gradient backpropagation as developed in last week's exercise and even provides the same attribute names:\n",
    "- ``data`` stores the internal data of the tensor representing its value.\n",
    "- ``requires_grad`` specifies if the tensor participates in the build-up of the computation graph.\n",
    "- ``grad`` keeps a tensor storing the gradient of the last backpropagation for leaf nodes of the graph with ``requires_grad=True``.\n",
    "- ``grad_fn`` points to the operation generating the tensor; ``None`` if the tensor is a leaf node.\n",
    "\n",
    "Again, the backpropagation is initiated by calling ``backward`` on a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., requires_grad=True)\n",
      "tensor(2.)\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor(1., requires_grad=True)\n",
    "b = torch.tensor(2.)\n",
    "print (a)\n",
    "print (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = a + b\n",
    "d = c + a\n",
    "d = d * d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "tensor(16.)\n",
      "<AddBackward0 object at 0x7f197d640cd0>\n",
      "<MulBackward0 object at 0x7f197d640c50>\n"
     ]
    }
   ],
   "source": [
    "print(b.grad)\n",
    "print(a.grad)\n",
    "print(c.grad_fn)\n",
    "print(d.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the PyTorch's agressive buffer freeing, computation graphs will not be held alive causing errors when calling ``backward`` multiple times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not working...\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    d.backward()\n",
    "except:\n",
    "    print(\"Not working...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At inference time one usually does not want to bulild up the computation graph for calculating the gradients as it's computationally expensive and bears the danger of memory leakage. Therefore PyTorch provides the context manager ``torch.no_grad`` for disabling gradient computations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nope, doesn't work.\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor(1., requires_grad=True)\n",
    "b = torch.tensor(2.)\n",
    "\n",
    "with torch.no_grad():\n",
    "    c = a + b\n",
    "\n",
    "try:\n",
    "    c.backward()\n",
    "except:\n",
    "    print(\"Nope, doesn't work.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until now, all computations in this exercise class were carried out on a CPU. The hardware of choice for Deep Learning applications, however, are GPUs. The high amount of parallelization and efficient calculation of simple operations on GPUs are highly beneficial for training and infering in artificial neural networks. Therefore, all major Deep Learning frameworks offer the possibility to utilize graphics cards.\n",
    "\n",
    "In PyTorch we can move a tensor to the GPU by simply calling the function ``cuda`` returning a pointer to the tensor. The device of a tensor can be found out using the ``device`` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor(2.)\n",
    "print(a.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = a.cuda()\n",
    "print(a.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above output ``cuda`` means that the tensor is located on a GPU and the number after the colon specifies the device among all available GPUs. During the course of this exercise class, you will only have to deal with one GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data are the driving force in the field of machine learning and therefore its of utmost important to deal with data properly. Luckily, PyTorch provides some handy tools which make the treatment of data very easy. The base class for representing data sets is ``torch.utils.data.Dataset``. This class overloads the functions ``__len__`` and ``__getitem__`` which allow us to easily access data contained in the set as we know it for example from Pythons lists.\n",
    "\n",
    "PyTorch already comprises wrapper classes for some poular data sets in ``torchvision.datasets``. To get started, let's use the famous MNIST data set for handwritten digits from LeCun et al. http://yann.lecun.com/exdb/mnist/. The argument ``transform`` below forces the data to be converted to PyTorch Tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = MNIST('data', download=True, transform=transforms.ToTensor())\n",
    "test_set = MNIST('data', train=False, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The single elements of these data sets are tuples containing an image representing a handwritten digit stored as an array in a PyTorch Tensor and the corresponding value of the digit as label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_set))\n",
    "print(len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n",
      "torch.Size([1, 28, 28])\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "el = train_set[2]                 #el is a tuple containing 2 list 1 image 2 its labels\n",
    "print(type(el))\n",
    "print(el[0].shape)                #image is again a sub list inside el so it is [1,28,28] so calling el[0][0] accesses the sblist image first then the 1st element inside the image this has now size 28x28\n",
    "print(el[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we know the value of the digit represented in the image is 5. We can show the image using ``matplotlib.pyplot.imshow``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Label: 4')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(el[0][0], cmap='gray', vmin=0., vmax=1.)   #calls 1st element of tuple, the image. then the 1st channel of the image\n",
    "plt.title('Label: {}'.format(el[1]))                  #calls 2ne element of the tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that ``imshow`` requires grayscale images to be of size $H\\times W$ but ``el`` is of size $1\\times H \\times W$. The reason therefor is that images are in general represented as $C \\times H \\times W$ arrays, where $C$ denotes the number of channels of the image. For color images $C$ typically amounts to $3$, for grayscale images to $1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equally important as the data itself is the process of loading it efficiently to the processing unit. Again, PyTorch provides an easy interface for tackling this problem. Most of the time, we also want mini-batches of data rather than single elements of our data set. The class ``DataLoader`` from ``torch.utils.data`` takes care of this and we only have to provide the data set and define the batch size. Additionally, we may specify further arguments like if we want the data to be shuffled or the number of workers involoved in the process of loading the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set, batch_size=4, num_workers=4, shuffle=True) #this is a list having all training elements in batches of 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at one of the mini-batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = iter(train_loader).next()   #batch is an iterator which gives a single list at a time. each list has 2 elements, element 0 is a list of 4 images and element 1 is a list of its labels\n",
    "\n",
    "data = batch[0]                     #this image is a 3D tensor => CxWxH which means image is again a list inside list\n",
    "labels = batch[1]\n",
    "print(data.shape)\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "torch.Size([4, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "print(type(batch))\n",
    "print(batch[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of the array has increased by the first dimension related to the differnt batch elements, i.e. the general shape for images in mini-batches is $B \\times C \\times H \\times W$ where $B$ is the batch size. Let's visualize the images of this mini-batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([28, 28])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([28, 28])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data.shape)\n",
    "print(data[0,0].shape)\n",
    "data [0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    plt.subplot(1, 4, i+1)\n",
    "    plt.imshow(data[i, 0], cmap='gray', vmin=0., vmax=1.)\n",
    "    plt.title('Label: {}'.format(labels[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the elements are randomly shuffled each time we iterate over the data loader."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to our ``toolbox.Module``, PyTorch defines ``Module`` in ``torch.nn`` which serves as base class for neural networks and their layers. For defining a network, you have to inherit from ``torch.nn.Module`` and define the ``forward`` function. Furthermore, ``torch.nn`` provides you with a broad variety of already implemented tools and layers you can use in your network. To get a first impression, have a look at the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(28*28, 20)\n",
    "        self.layer2 = nn.ReLU()\n",
    "        self.layer3 = nn.Linear(20, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)   #If there is any situation that you don't know how many rows(no of samples) you want but are sure of the number of columns, then you can specify this with a -1.\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go through this: In the constructor we instantiate a couple of layers we want to use in the forward pass. For the linear layer we have to specifty the input and output dimensions of our linear transformations. The batch sizes usually are excluded from these size considerations as the data is procesed separately for each element in the mini-batch. To shed light on this mechanism, consider ``layer3``. It expects inputs of size $B \\times 20$, multiplies each element of the batch with a matrix of size $10 \\times 20$ (and possibly adds some vector afterwards) and outputs the transformed data of size $B \\times 10$.\n",
    "\n",
    "In order to be able to pass an image to a linear layer, we have to resize the image with the dimensions $B \\times C \\times H \\times W$ to a vector of size $B \\times \\left( C \\cdot H \\cdot W \\right)$. The analogy of NumPy's ``reshape`` is the function ``view`` in PyTorch. The forward pass of an object of type ``torch.nn.Module`` can be performed by simply calling the object itself like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = SimpleNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 10])\n"
     ]
    }
   ],
   "source": [
    "#FORWARDpass\n",
    "with torch.no_grad():\n",
    "    output = network(batch[0])     #4 images of a batch will be parallelly(processing) going through forward pass\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3, 2, 2, 5])\n",
      "tensor([5, 3, 0, 4])\n"
     ]
    }
   ],
   "source": [
    "#Predicting the o/p after 1 fwd pass, it has 4 images being parallely processed\n",
    "_, pred_labels = output.max(dim=1)   #max along the column i.e, dim=1. max in each of the 4 elements in the batch \n",
    "                                     #each element has 10 values we find max of this list\n",
    "print(pred_labels)                   #then the label index corresponding to this max value is printed, which shows to which class it falls\n",
    "print(labels)                        #this prints the actual labels of the 4 i/p images "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have set the output of the last layer to be of size ten as we want to predict one out of ten labels. We would like our network to assign the highest value of the output on the coordinate corresponding to the correct label. As discussed earlier on, cross entropy loss is suitable for classification tasks penalizing high output values on the wrong labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fun = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.2899)\n"
     ]
    }
   ],
   "source": [
    "loss = loss_fun(output, labels)       #calculating loss by using o/p of layer and the ground truth labels\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As soon as it comes to training, we want to modify the network's parameters in order to decrease the loss on the training set. We can access the learnable parameters of a network by calling its ``parameters`` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#accessing the wights/parameters after one fwd pass\n",
    "params = network.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 784])\n",
      "torch.Size([20])\n",
      "torch.Size([10, 20])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for param in params:\n",
    "    print(param.shape)\n",
    "#shape [20,784] means 1st layer has 20 neurons and 784 element of 1 image goes to each of 20 neurons\n",
    "#then 20 nuerons means 20 bias\n",
    "#2nd layer [10,20] weights so in total 200params\n",
    "#2nd layer 10 bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case the parameters of the network are the weights and biases of the two linear layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding multiple layers to a network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the number of layers increases, it gets tedious to add every single layer to the attribute list of the network and explicitly iterate over them in the forward pass. A convenient alternative in this case is PyTorch's container class ``torch.nn.Sequential`` which allows you to condense multiple layers into a single ``Module`` object. The usage is straight-forward as you can see in the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet2(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(nn.Linear(28*28, 20), nn.ReLU(), nn.Linear(20, 10))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = self.layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** It is not sufficient to store the network's layers in a list as we did in our own ``Network`` class from the second last exercise class. Even though you would still be able to perform a forward pass, the layers are not registered as submodules of the network object and therefore their parameters can't be accessed using the ``parameters`` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to perform training or inference of a network on a GPU we have to move the data as well as the network itself to the GPU. This again works by simply calling the ``cuda`` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels = data.cuda(), labels.cuda()\n",
    "network = network.cuda()\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = network(data)\n",
    "    loss = loss_fun(output, labels)\n",
    "\n",
    "print(output.device)\n",
    "print(loss.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a network using SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement SGD in ``optimizer.py`` as introduced in the lecture and do not use the SGD otimizer included in PyTorch. Test your implementation by running the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimizer import SGD\n",
    "import pickle\n",
    "\n",
    "net = SimpleNet2()\n",
    "\n",
    "net.load_state_dict(torch.load('net_params'))\n",
    "\n",
    "batch = next(iter(DataLoader(train_set, batch_size=4)))\n",
    "\n",
    "op = net(batch[0])\n",
    "\n",
    "l = loss_fun(op, batch[1])\n",
    "\n",
    "l.backward()\n",
    "\n",
    "optim = SGD(net.parameters())\n",
    "\n",
    "optim.step()\n",
    "\n",
    "with open('net_params_updated', 'rb') as file:\n",
    "    params = pickle.load(file)\n",
    "\n",
    "for i, param in enumerate(list(net.parameters())):\n",
    "    assert (param - params[i]).abs().max() < 1e-6\n",
    "\n",
    "optim.zero_grad()\n",
    "\n",
    "for param in net.parameters():\n",
    "    assert param.grad.abs().max() == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
