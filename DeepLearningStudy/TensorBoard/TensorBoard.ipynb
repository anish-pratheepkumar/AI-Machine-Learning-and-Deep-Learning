{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embed (Embedding)            (None, 500, 128)          256000    \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 494, 32)           28704     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 98, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 92, 32)            7200      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 291,937\n",
      "Trainable params: 291,937\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "#data preprocessing\n",
    "max_features = 2000                                                                        #maximum index of words i.e, the integer code for the words per sample (cuts sample size to most common 2000). the resulting sample can have any length may be even 2000 maximum\n",
    "max_len = 500                                                                              #then the length of each sample(most common 500 words are selected). the indexes in the resulting sample can be any value from 0 to 2000 but only 500 elements(indexes) will be there in each sample\n",
    "#imdb data set has 25000 samples each for test and train sets\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)              #loads data with each sample having 20000words\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=max_len)                                  #again truncates/pads the data with most common 500words per sample\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=max_len)\n",
    "\n",
    "#designing network architecture\n",
    "model = keras.models.Sequential()\n",
    "#mbedding converts +ve integers to dense vectors of fixed size\n",
    "model.add(layers.Embedding(max_features,                                     #maximum integer value of the indexes of words in each sample(2000)\n",
    "                           128,                                              #o/p dimension\n",
    "                           input_length=max_len,                             #number of indexes or integers in each sample\n",
    "                           name='embed'))                                    #so here i/p is each sample with 500 integers and each sample o/p is of size 500x128 \n",
    "model.add(layers.Conv1D(32, 7, activation='relu'))                           #o/p last dimension is 32, and each kernel is a 1dvector of size 7\n",
    "model.add(layers.MaxPooling1D(5))\n",
    "model.add(layers.Conv1D(32, 7, activation='relu'))\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "model.add(layers.Dense(1))\n",
    "model.summary()\n",
    "\n",
    "#configuring the model\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 500)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a directory for TensorBoard log files\n",
    "import os\n",
    "base_dir = '/home/anish/Documents/Jupyter Notebook/SA-DLwithPy/TensorBoard/my_log_dir'\n",
    "#os.mkdir(base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anish/anaconda2/envs/cpu2.7/lib/python2.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 41s 2ms/step - loss: 7.7364 - acc: 0.4985 - val_loss: 7.6168 - val_acc: 0.5062\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 40s 2ms/step - loss: 7.7364 - acc: 0.4985 - val_loss: 7.6168 - val_acc: 0.5062\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 38s 2ms/step - loss: 7.7364 - acc: 0.4985 - val_loss: 7.6168 - val_acc: 0.5062\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 37s 2ms/step - loss: 7.7364 - acc: 0.4985 - val_loss: 7.6168 - val_acc: 0.5062\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 39s 2ms/step - loss: 7.7364 - acc: 0.4985 - val_loss: 7.6168 - val_acc: 0.5062\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 38s 2ms/step - loss: 7.7364 - acc: 0.4985 - val_loss: 7.6168 - val_acc: 0.5062\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 36s 2ms/step - loss: 7.7364 - acc: 0.4985 - val_loss: 7.6168 - val_acc: 0.5062\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 36s 2ms/step - loss: 7.7364 - acc: 0.4985 - val_loss: 7.6168 - val_acc: 0.5062\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 36s 2ms/step - loss: 7.7364 - acc: 0.4985 - val_loss: 7.6168 - val_acc: 0.5062\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 36s 2ms/step - loss: 7.7364 - acc: 0.4985 - val_loss: 7.6168 - val_acc: 0.5062\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 36s 2ms/step - loss: 7.7364 - acc: 0.4985 - val_loss: 7.6168 - val_acc: 0.5062\n",
      "Epoch 12/20\n",
      "20000/20000 [==============================] - 36s 2ms/step - loss: 7.7364 - acc: 0.4985 - val_loss: 7.6168 - val_acc: 0.5062\n",
      "Epoch 13/20\n",
      "20000/20000 [==============================] - 36s 2ms/step - loss: 7.7364 - acc: 0.4985 - val_loss: 7.6168 - val_acc: 0.5062\n",
      "Epoch 14/20\n",
      "20000/20000 [==============================] - 36s 2ms/step - loss: 7.7364 - acc: 0.4985 - val_loss: 7.6168 - val_acc: 0.5062\n",
      "Epoch 15/20\n",
      "20000/20000 [==============================] - 36s 2ms/step - loss: 7.7364 - acc: 0.4985 - val_loss: 7.6168 - val_acc: 0.5062\n",
      "Epoch 16/20\n",
      "20000/20000 [==============================] - 36s 2ms/step - loss: 7.7364 - acc: 0.4985 - val_loss: 7.6168 - val_acc: 0.5062\n",
      "Epoch 17/20\n",
      "20000/20000 [==============================] - 36s 2ms/step - loss: 7.7364 - acc: 0.4985 - val_loss: 7.6168 - val_acc: 0.5062\n",
      "Epoch 18/20\n",
      "20000/20000 [==============================] - 36s 2ms/step - loss: 7.7364 - acc: 0.4985 - val_loss: 7.6168 - val_acc: 0.5062\n",
      "Epoch 19/20\n",
      "20000/20000 [==============================] - 36s 2ms/step - loss: 7.7364 - acc: 0.4985 - val_loss: 7.6168 - val_acc: 0.5062\n",
      "Epoch 20/20\n",
      "20000/20000 [==============================] - 36s 2ms/step - loss: 7.7364 - acc: 0.4985 - val_loss: 7.6168 - val_acc: 0.5062\n"
     ]
    }
   ],
   "source": [
    "#adding tensorboard callback\n",
    "callbacks = [\n",
    "    keras.callbacks.TensorBoard(\n",
    "        log_dir='my_log_dir')\n",
    "]\n",
    " \n",
    "#training the model with TensorBoard callback   \n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=128,\n",
    "                    validation_split=0.2,\n",
    "                    callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
