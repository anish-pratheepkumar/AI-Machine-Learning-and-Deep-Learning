{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embed (Embedding)            (None, 500, 128)          256000    \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 494, 32)           28704     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 98, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 92, 32)            7200      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 291,937\n",
      "Trainable params: 291,937\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "#data preprocessing\n",
    "max_features = 2000                                                                        #maximum index of words i.e, the integer code for the words per sample (cuts sample size to most common 2000). the resulting sample can have any length may be even 2000 maximum\n",
    "max_len = 500                                                                              #then the length of each sample(most common 500 words are selected). the indexes in the resulting sample can be any value from 0 to 2000 but only 500 elements(indexes) will be there in each sample\n",
    "#imdb data set has 25000 samples each for test and train sets\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)              #loads data with each sample having 20000words\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=max_len)                                  #again truncates/pads the data with most common 500words per sample\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=max_len)\n",
    "\n",
    "#designing network architecture\n",
    "model = keras.models.Sequential()\n",
    "#mbedding converts +ve integers to dense vectors of fixed size\n",
    "model.add(layers.Embedding(max_features,                                     #maximum integer value of the indexes of words in each sample(2000)\n",
    "                           128,                                              #o/p dimension\n",
    "                           input_length=max_len,                             #number of indexes or integers in each sample\n",
    "                           name='embed'))                                    #so here i/p is each sample with 500 integers and each sample o/p is of size 500x128 \n",
    "model.add(layers.Conv1D(32, 7, activation='relu'))                           #o/p last dimension is 32, and each kernel is a 1dvector of size 7\n",
    "model.add(layers.MaxPooling1D(5))\n",
    "model.add(layers.Conv1D(32, 7, activation='relu'))\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "model.add(layers.Dense(1))\n",
    "model.summary()\n",
    "\n",
    "#configuring the model\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 500)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a directory for TensorBoard log files\n",
    "import os\n",
    "base_dir = '/home/anish/Documents/Jupyter Notebook/SA-DLwithPy/TensorBoard/my_log_dir'\n",
    "#os.mkdir(base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 41s 2ms/step - loss: 0.3597 - acc: 0.7480 - val_loss: 0.4865 - val_acc: 0.7482\n",
      "WARNING:tensorflow:From /home/anish/Softwares/Anaconda2Software/lib/python2.7/site-packages/keras/callbacks/tensorboard_v1.py:343: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.\n",
      "\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 47s 2ms/step - loss: 0.2949 - acc: 0.7596 - val_loss: 0.5618 - val_acc: 0.7004\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 44s 2ms/step - loss: 0.2744 - acc: 0.6739 - val_loss: 0.5918 - val_acc: 0.6306\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 43s 2ms/step - loss: 0.2283 - acc: 0.6388 - val_loss: 0.7066 - val_acc: 0.5734\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 44s 2ms/step - loss: 0.1866 - acc: 0.5946 - val_loss: 0.7477 - val_acc: 0.4994\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 46s 2ms/step - loss: 0.1599 - acc: 0.5293 - val_loss: 0.8173 - val_acc: 0.4566\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 45s 2ms/step - loss: 0.1374 - acc: 0.4445 - val_loss: 0.8993 - val_acc: 0.3938\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 42s 2ms/step - loss: 0.1223 - acc: 0.3672 - val_loss: 1.0086 - val_acc: 0.3368\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 42s 2ms/step - loss: 0.1206 - acc: 0.3137 - val_loss: 1.0210 - val_acc: 0.3164\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 49s 2ms/step - loss: 0.1088 - acc: 0.2851 - val_loss: 1.3606 - val_acc: 0.2692\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 45s 2ms/step - loss: 0.1086 - acc: 0.2348 - val_loss: 1.1229 - val_acc: 0.2654\n",
      "Epoch 12/20\n",
      "20000/20000 [==============================] - 41s 2ms/step - loss: 0.1046 - acc: 0.1998 - val_loss: 1.0990 - val_acc: 0.2582\n",
      "Epoch 13/20\n",
      "20000/20000 [==============================] - 41s 2ms/step - loss: 0.1136 - acc: 0.1831 - val_loss: 1.1603 - val_acc: 0.2500\n",
      "Epoch 14/20\n",
      "20000/20000 [==============================] - 41s 2ms/step - loss: 0.1019 - acc: 0.1542 - val_loss: 1.2314 - val_acc: 0.2270\n",
      "Epoch 15/20\n",
      "20000/20000 [==============================] - 41s 2ms/step - loss: 0.1005 - acc: 0.1374 - val_loss: 1.1962 - val_acc: 0.2216\n",
      "Epoch 16/20\n",
      "20000/20000 [==============================] - 41s 2ms/step - loss: 0.1020 - acc: 0.1306 - val_loss: 1.2381 - val_acc: 0.2066\n",
      "Epoch 17/20\n",
      "20000/20000 [==============================] - 41s 2ms/step - loss: 0.1038 - acc: 0.1178 - val_loss: 1.2746 - val_acc: 0.2040\n",
      "Epoch 18/20\n",
      "20000/20000 [==============================] - 41s 2ms/step - loss: 0.1012 - acc: 0.1036 - val_loss: 1.3102 - val_acc: 0.2066\n",
      "Epoch 19/20\n",
      "20000/20000 [==============================] - 41s 2ms/step - loss: 0.1010 - acc: 0.1017 - val_loss: 1.3114 - val_acc: 0.1968\n",
      "Epoch 20/20\n",
      "20000/20000 [==============================] - 41s 2ms/step - loss: 0.0994 - acc: 0.0976 - val_loss: 1.3232 - val_acc: 0.1920\n"
     ]
    }
   ],
   "source": [
    "#adding tensorboard callback\n",
    "callbacks = [\n",
    "    keras.callbacks.TensorBoard(\n",
    "        log_dir='my_log_dir')\n",
    "]\n",
    " \n",
    "#training the model with TensorBoard callback   \n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=128,\n",
    "                    validation_split=0.2,\n",
    "                    callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
