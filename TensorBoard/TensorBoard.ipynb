{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embed (Embedding)            (None, 500, 128)          256000    \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 494, 32)           28704     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 98, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 92, 32)            7200      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 291,937\n",
      "Trainable params: 291,937\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "#data preprocessing\n",
    "max_features = 2000                                                                        #maximum index of words i.e, the integer code for the words per sample (cuts sample size to most common 2000). the resulting sample can have any length may be even 2000 maximum\n",
    "max_len = 500                                                                              #then the length of each sample(most common 500 words are selected). the indexes in the resulting sample can be any value from 0 to 2000 but only 500 elements(indexes) will be there in each sample\n",
    "#imdb data set has 25000 samples each for test and train sets\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)              #loads data with each sample having 20000words\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=max_len)                                  #again truncates/pads the data with most common 500words per sample\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=max_len)\n",
    "\n",
    "#designing network architecture\n",
    "model = keras.models.Sequential()\n",
    "#mbedding converts +ve integers to dense vectors of fixed size\n",
    "model.add(layers.Embedding(max_features,                                     #maximum integer value of the indexes of words in each sample(2000)\n",
    "                           128,                                              #o/p dimension\n",
    "                           input_length=max_len,                             #number of indexes or integers in each sample\n",
    "                           name='embed'))                                    #so here i/p is each sample with 500 integers and each sample o/p is of size 500x128 \n",
    "model.add(layers.Conv1D(32, 7, activation='relu'))                           #o/p last dimension is 32, and each kernel is a 1dvector of size 7\n",
    "model.add(layers.MaxPooling1D(5))\n",
    "model.add(layers.Conv1D(32, 7, activation='relu'))\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "model.add(layers.Dense(1))\n",
    "model.summary()\n",
    "\n",
    "#configuring the model\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 500)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a directory for TensorBoard log files\n",
    "#mkdir my_log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anish/anaconda2/envs/cpu2.7/lib/python2.7/site-packages/keras/callbacks/tensorboard_v2.py:102: UserWarning: The TensorBoard callback does not support embeddings display when using TensorFlow 2.0. Embeddings-related arguments are ignored.\n",
      "  warnings.warn('The TensorBoard callback does not support '\n",
      "/home/anish/anaconda2/envs/cpu2.7/lib/python2.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "15616/20000 [======================>.......] - ETA: 10s - loss: 7.7125 - acc: 0.5000"
     ]
    }
   ],
   "source": [
    "embeddings_freq=1,#training the model with TensorBoard callback\n",
    "callbacks = [\n",
    "    keras.callbacks.TensorBoard(\n",
    "        log_dir='my_log_dir',\n",
    "        histogram_freq=1,                                                  #records activation histograms every 1 epoch\n",
    "        embeddings_freq=1)                                                 #records embeding data every 1 epoch\n",
    "]\n",
    "\n",
    "#Training the model\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=128,\n",
    "                    validation_split=0.2,\n",
    "                    callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
